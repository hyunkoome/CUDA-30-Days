#include <torch/extension.h>
#include <cuda_runtime.h>
#include <cuda_fp16.h>  // â¬…ï¸ half ìë£Œí˜• ì§€ì›
#include <vector>

#define THREADS 256  // CUDA blockë‹¹ ì“°ë ˆë“œ ìˆ˜

/*
í•¨ìˆ˜ ì´ë¦„       	ë°©ì‹              	ì…ë ¥ ìë£Œí˜• â†’ ì¶œë ¥ ìë£Œí˜•
dot_shared()	Shared Memory	    float32 â†’ float32
dot_double()	Double Precision	float64 â†’ float64
dot_warp()	    Warp Shuffle	    float32 â†’ float32
dot_half()	    Mixed Precision	    float16 â†’ float32

ğŸ”§ ì£¼ì˜ ì‚¬í•­
    PyTorchì—ì„œ float16ì€ at::Halfì´ì§€ë§Œ, ì»¤ë„ì—ì„œëŠ” __halfì´ë¯€ë¡œ reinterpret_cast í•„ìš”
    dot_half() ê²°ê³¼ëŠ” float32 í…ì„œë¡œ ë°˜í™˜ë©ë‹ˆë‹¤ (ì—°ì‚° ì •í™•ë„ ë•Œë¬¸).
    ì»¤ë„ dotProductHalfê°€ float ê²°ê³¼ë¥¼ atomicAddë¡œ ë°˜í™˜í•˜ëŠ” êµ¬ì¡°ì´ê¸° ë•Œë¬¸ì—, wrapperì—ì„œë„ float32ë¡œ ë°›ëŠ” ê²ƒì´ ë§ìŠµë‹ˆë‹¤.
*/

// ----------------------------------------------------
// ì™¸ë¶€ CUDA ì»¤ë„ ì„ ì–¸ (ê° .cu íŒŒì¼ì—ì„œ ì •ì˜ë¨)
// ----------------------------------------------------
extern "C" __global__ void dotProductShared(const float* a, const float* b, float* result, int size);
extern "C" __global__ void dotProductDouble(const double* a, const double* b, double* result, int size);
extern "C" __global__ void dotProductWarp(const float* a, const float* b, float* result, int size);
extern "C" __global__ void dotProductHalf(const __half* a, const __half* b, float* result, int size);  // âœ… ì¶”ê°€ë¨

// -----------------------------------------------------
// [1] Shared Memory ë²„ì „ ë‚´ì  ì—°ì‚° í•¨ìˆ˜ (float32 ì „ìš©)
// -----------------------------------------------------
torch::Tensor dot_shared(torch::Tensor a, torch::Tensor b) {
    const int size = a.numel();
    const int blocks = (size + THREADS - 1) / THREADS;

    auto result = torch::zeros({1}, a.options());  // float32 ê²°ê³¼

    dotProductShared<<<blocks, THREADS>>>(
        a.data_ptr<float>(),
        b.data_ptr<float>(),
        result.data_ptr<float>(),
        size
    );

    return result;
}

// --------------------------------------------------------
// [2] Double Precision ë²„ì „ ë‚´ì  ì—°ì‚° í•¨ìˆ˜ (float64 ì „ìš©)
// --------------------------------------------------------
torch::Tensor dot_double(torch::Tensor a, torch::Tensor b) {
    const int size = a.numel();
    const int blocks = (size + THREADS - 1) / THREADS;

    auto result = torch::zeros({1}, a.options().dtype(torch::kFloat64));

    dotProductDouble<<<blocks, THREADS>>>(
        a.data_ptr<double>(),
        b.data_ptr<double>(),
        result.data_ptr<double>(),
        size
    );

    return result;
}

// ------------------------------------------------------
// [3] Warp Shuffle ë²„ì „ ë‚´ì  ì—°ì‚° í•¨ìˆ˜ (float32 ì „ìš©)
// ------------------------------------------------------
torch::Tensor dot_warp(torch::Tensor a, torch::Tensor b) {
    const int size = a.numel();
    const int blocks = (size + THREADS - 1) / THREADS;

    auto result = torch::zeros({1}, a.options());

    dotProductWarp<<<blocks, THREADS>>>(
        a.data_ptr<float>(),
        b.data_ptr<float>(),
        result.data_ptr<float>(),
        size
    );

    return result;
}

// -------------------------------------------------------------
// [4] Mixed Precision ë²„ì „ ë‚´ì  ì—°ì‚° í•¨ìˆ˜ (float16 â†’ float32 ì¶œë ¥)
// -------------------------------------------------------------
torch::Tensor dot_half(torch::Tensor a, torch::Tensor b) {
    const int size = a.numel();
    const int blocks = (size + THREADS - 1) / THREADS;

    // float16 ì…ë ¥ â†’ float32 ì¶œë ¥
    auto result = torch::zeros({1}, a.options().dtype(torch::kFloat32));

    dotProductHalf<<<blocks, THREADS>>>(
        reinterpret_cast<const __half*>(a.data_ptr<at::Half>()),  // â¬…ï¸ PyTorch half â†’ CUDA half
        reinterpret_cast<const __half*>(b.data_ptr<at::Half>()),
        result.data_ptr<float>(),
        size
    );

    return result;
}

// ----------------------------
// PyTorch í™•ì¥ ë°”ì¸ë”© ë“±ë¡
// ----------------------------
PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("dot_shared", &dot_shared, "Dot Product (Shared Memory, float32)");
    m.def("dot_double", &dot_double, "Dot Product (Double Precision, float64)");
    m.def("dot_warp",   &dot_warp,   "Dot Product (Warp Shuffle, float32)");
    m.def("dot_half",   &dot_half,   "Dot Product (Mixed Precision, float16â†’float32)");
}

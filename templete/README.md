# π€ Day XX - [μ—°μ‚°μ΄λ¦„] CUDA μ»¤λ„ κµ¬ν„

## π“ λ©ν‘

- CUDAλ΅ `[μ—°μ‚°μ΄λ¦„]` μ»¤λ„μ„ μ§μ ‘ κµ¬ν„ν•©λ‹λ‹¤.
- PyTorchμ™€ C++μ—μ„ μ—°λ™ν•μ—¬ μ •ν™•μ„± κ²€μ¦μ„ μν–‰ν•©λ‹λ‹¤.
- κΈ°μ΅΄ PyTorch μ—°μ‚°κ³Ό μ„±λ¥μ„ λΉ„κµν•©λ‹λ‹¤.

---

## π“ νμΌ κµ¬μ„±

| νμΌλ… | μ„¤λ… |
|--------|------|
| `op_kernel.cu` | CUDA μ»¤λ„ λ©”μΈ κµ¬ν„ νμΌ |
| `op_bindings.py` | PyTorchμ—μ„ CUDA μ»¤λ„μ„ λ¶λ¬μ¤λ” ν™•μ¥ λ¨λ“ |
| `test_python_runner.py` | PyTorchμ™€ κ²°κ³Ό λΉ„κµ λ° μ •ν™•λ„ κ²€μ¦ |
| `test_runner.cpp` | C++ ν™κ²½μ—μ„ μ§μ ‘ CUDA μ‹¤ν–‰ κ²°κ³Ό ν™•μΈ |
| `op_cpu.cpp` (optional) | CPU λΉ„κµμ© ν•¨μ (μ •ν™•λ„ μ°Έκ³ ) |
| `op_onnx.py` (optional) | ONNX export μ© μ½”λ“ |
| `op_trt.cpp` (optional) | TensorRT Plugin μ—°λ™ μ‹¤ν—μ© μ½”λ“ |
| `op_webgpu.wgsl` (optional) | WebGPU λ€μ‘ WGSL κµ¬ν„ |
| `README.md` | μ‹¤μµ μ”μ•½ λ¬Έμ„ (ν„μ¬ νμΌ) |

---

## π§ μ‹¤ν–‰ λ°©λ²•

### β–¶ Python (PyTorch μ—°λ™)

```bash
python test_python_runner.py
```

> κ²°κ³Ό: PyTorch μ—°μ‚°κ³Ό λΉ„κµν•μ—¬ `torch.allclose()` κ²€μ¦

---

### β–¶ C++ (λ‹¨λ… μ‹¤ν–‰)

```bash
nvcc -o test_runner test_runner.cpp op_kernel.cu
./test_runner
```

> κ²°κ³Ό: C++ λ‚΄μ—μ„ μ§μ ‘ μ‹¤ν–‰ ν›„ μμΉ μ¶λ ¥ ν™•μΈ

---

### β–¶ ONNX (μ„ νƒ)

```bash
python op_onnx.py
```

> ONNXRuntime λλ” NetronμΌλ΅ κµ¬μ΅° ν™•μΈ κ°€λ¥

---

## π” κ²°κ³Ό λΉ„κµ μμ‹

| ν•­λ© | PyTorch | CUDA | μ°¨μ΄ μ—¬λ¶€ |
|------|---------|------|------------|
| κ²°κ³Ό κ°’ | `[0.1, 0.2]` | `[0.1001, 0.2001]` | PASS |
| μ‹¤ν–‰ μ‹κ°„(ms) | `12.4` | `3.1` | +4x |
| μ •ν™•λ„ λΉ„κµ | `MAE < 1e-4` |  | OK |

---

## π“ μ£Όμ” κ°λ… μ •λ¦¬

- **μ¤λ λ“ κµ¬μ΅°**: `blockIdx`, `threadIdx`, `gridDim` λ“± CUDA λ³‘λ ¬μ²λ¦¬ κµ¬μ΅° μ΄ν•΄
- **λ©”λ¨λ¦¬ ν™μ©**: global vs shared memory, coalesced access
- **μµμ ν™” ν¬μΈνΈ**: `__syncthreads()`, tiling, unrolling λ“±

---

## π§  μƒκ°ν•΄λ³΄κΈ°

- μ΄ μ»¤λ„μ—μ„ shared memoryλ¥Ό μ‚¬μ©ν•λ©΄ μ„±λ¥μ΄ λ” ν–¥μƒλ κΉ?
- μ΄ μ—°μ‚°μ€ WebGPUμ—μ„λ„ μ μ‚¬ν• λ°©μ‹μΌλ΅ κµ¬ν„ κ°€λ¥ν•κ°€?
- μ΄ μ»¤λ„μ€ ONNX β†’ TensorRT λ³€ν™ μ‹ μ—°μ‚° μ¬ν™μ©μ΄ κ°€λ¥ν• κΉ?

---
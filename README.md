# CUDA-30-Days

# ğŸ”¥ CUDA Kernel Study: Conv + Transformer + Attention + SPConv

> 30ì¼ê°„ì˜ CUDA ì»¤ë„ ì§ì ‘ êµ¬í˜„ê³¼ PyTorch/C++/TensorRT/WebGPU ì—°ë™ê¹Œì§€  
> Conv2D, Transformer, Attention, SparseConv í•µì‹¬ ì—°ì‚°ì„ í•œ ë²ˆì— ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤.

---

## ğŸ“Œ ëª©í‘œ

- CUDA ì»¤ë„ ê¸°ë³¸ ê°œë… ë° ì‹¤ìŠµ
- Conv/Transformer/Attention ì»¤ë„ ì§ì ‘ êµ¬í˜„
- Python(PyTorch) ë° C++ ì—°ë™ í…ŒìŠ¤íŠ¸
- PyTorch/ONNX/TensorRT/WebGPU ë¹„êµ ë¶„ì„

---

## ğŸ› ï¸ ì‹¤ìŠµ í™˜ê²½

| êµ¬ì„± | ë²„ì „                               |
|------|----------------------------------|
| CUDA | 12.6                             |
| PyTorch | 2.x+, `ì—¬ê¸°ì—ì„œëŠ” 2.3.0 (CUDA 12.6 ì „ìš©)` |
| Python | 3.10+                            |
| C++ | CMake ë˜ëŠ” g++                     |
| ì„ íƒì‚¬í•­ | ONNXRuntime, TensorRT, WebGPU í™˜ê²½ |

# Python 3.10ì´ ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ëŠ” ì „ì œ í•˜ì—
```
python3.10 -m venv cudastudy
source cudastudy/bin/activate   # Windows: cudastudy\\Scripts\\activate
pip install --upgrade pip
pip install -r requirements.txt
```

# libtorch ì„¤ì¹˜ 
PyTorchëŠ” CUDA minor ë²„ì „ì´ ë‹¬ë¼ë„ ë³´í†µ ì˜ ë™ì‘í•©ë‹ˆë‹¤.
CUDA 12.6ì—ì„œ CUDA 12.1 ë¹Œë“œëœ libtorch ì‚¬ìš© â†’ âœ”ï¸ ì‘ë™ í™•ì¸ë¨
```shell
wget https://download.pytorch.org/libtorch/cu121/libtorch-cxx11-abi-shared-with-deps-2.1.0%2Bcu121.zip
unzip libtorch-cxx11-abi-shared-with-deps-2.1.0+cu121.zip -d ~/
```
~/libtorch ê²½ë¡œ ì•„ë˜ì—ëŠ” ë³´í†µ ì´ëŸ° êµ¬ì¡°ê°€ ìƒê¹€:
```shell
~/libtorch/
â”œâ”€â”€ include/
â”œâ”€â”€ lib/
â”œâ”€â”€ share/
â””â”€â”€ ...
```

---

## ğŸ—“ï¸ 30ì¼ ì»¤ë¦¬í˜ëŸ¼

| Day | ì»¤ë„ ì´ë¦„                                        | í•µì‹¬ ë‚´ìš© | ì—°ë™ ë° ë¹„êµ |
|-----|----------------------------------------------|-----------|---------------|
| 01 | [vec_add](./day01_vec_add/README.md)         | ë²¡í„° ë§ì…ˆ | PyTorch + C++ |
| 02 | [dot_product](./day02_dot_product/README.md) | ë‚´ì , reduction | PyTorch + C++ |
| 03 | [matrix_add](./day03_matrix_add/README.md)   | 2D í–‰ë ¬ í•© | PyTorch + C++ |
| 04 | relu                                         | í™œì„±í™” í•¨ìˆ˜ | PyTorch |
| 05 | matrix_transpose                             | ì „ì¹˜ | memory coalescing |
| 06 | matmul_basic                                 | naive GEMM | PyTorch |
| 07 | matmul_sharedmem                             | tiling | PyTorch + ì„±ëŠ¥ |
| 08 | global_avg_pool                              | GAP | PyTorch ë¹„êµ |
| 09 | batch_norm_forward                           | ì •ê·œí™” | ONNX export |
| 10 | instance_norm                                | ì±„ë„ë³„ ì •ê·œí™” | PyTorch |
| 11 | group_norm                                   | ê·¸ë£¹ ì •ê·œí™” | TensorRT plugin |
| 12 | layernorm_forward                            | LayerNorm FWD | PyTorch |
| 13 | layernorm_backward                           | LayerNorm BWD | ì„±ëŠ¥ íŠœë‹ |
| 14 | softmax_kernel                               | softmax | PyTorch |
| 15 | softmax_2d_kernel                            | Attentionìš© | PyTorch |
| 16 | einsum_qk_attn                               | QK^T | Attention ì—°ì‚° |
| 17 | attention_softmax_fused                      | fused attention | íš¨ìœ¨ ë¹„êµ |
| 18 | gelu_kernel                                  | GELU | activation ì„±ëŠ¥ |
| 19 | topk_kernel                                  | Top-K | PyTorch |
| 20 | prefix_sum_kernel                            | scan | warp-level sum |
| 21 | warp_shuffle_max                             | warp max | warp shuffle |
| 22 | conv2d_basic                                 | naive Conv | PyTorch |
| 23 | conv2d_shared_tile                           | tile + shared | PyTorch |
| 24 | conv2d_unroll                                | unroll ìµœì í™” | ì„±ëŠ¥ ë¹„êµ |
| 25 | spconv_sparse_gemm                           | sparse matmul | PyTorch |
| 26 | spconv_forward                               | sparse forward | TensorRT |
| 27 | spconv_backward                              | grad êµ¬í˜„ | ì‹¤ìŠµ |
| 28 | transformer_block                            | full block | residual í¬í•¨ |
| 29 | flash_attention                              | FlashAttention | memory opt |
| 30 | final_review_day                             | ì „ì²´ ì •ë¦¬ | ì„±ëŠ¥ ë¶„ì„ |

---

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì˜ˆì‹œ
```bash
cuda-kernel-study/
â”œâ”€â”€ README.md
â”œâ”€â”€ day01_vec_add/
â”‚ â”œâ”€â”€ vec_add_kernel.cu
â”‚ â”œâ”€â”€ vec_add_bindings.py
â”‚ â”œâ”€â”€ test_python_runner.py
â”‚ â”œâ”€â”€ test_runner.cpp
â”‚ â””â”€â”€ README.md
â”œâ”€â”€ day02_dot_product/
â”‚ â”œâ”€â”€ ...
```

## âœ… ì‹¤ìŠµ ë°©ì‹

1. CUDA ì»¤ë„ ì§ì ‘ êµ¬í˜„ (`*.cu`)
2. Python(PyTorch) ì—°ë™ (`*.py`)
3. C++ ì‹¤í–‰ ë¹„êµ (`test_runner.cpp`)
4. PyTorch / TensorRT / ONNX / WebGPU ê²°ê³¼ì™€ ë¹„êµ
5. ì„±ëŠ¥ ì¸¡ì • ë° ê²°ê³¼ ì •ë¦¬ (`README.md`)

---

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ì˜ˆì‹œ

| ì—°ì‚° | PyTorch(ms) | CUDA(ms) | ì„±ëŠ¥ í–¥ìƒ |
|------|-------------|----------|------------|
| matmul | 12.5 | 3.2 | â†‘ 3.9x |
| softmax | 4.3 | 1.8 | â†‘ 2.4x |

---

## ğŸ§  Special Thanks

- ë‹¤ì–‘í•œ ì—°ì‚°ë“¤ì„ ìµœì í™”í•˜ê³  ì§ì ‘ ë¹„êµí•˜ë©°, 
- ì‹¤ë¬´ì— ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì‹¤ë ¥ì„ ê¸°ë¥´ê¸° ìœ„í•´, 
- ì‹¤ì „ AI ì»¤ë„ ì—”ì§€ë‹ˆì–´ë§ì„ ëª©í‘œë¡œ êµ¬ì„±ë˜ì—ˆìŒ
---

## ğŸ“„ [ë¼ì´ì„ ìŠ¤ / License](LICENSE)
[![CC BY-NC 4.0](https://licensebuttons.net/l/by-nc/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc/4.0/)

- ë³¸ í”„ë¡œì íŠ¸ëŠ” **Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)** ë¼ì´ì„ ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.  
- ëˆ„êµ¬ë‚˜ **ë¹„ì˜ë¦¬ ëª©ì **ì— í•œí•´ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, **ìƒì—…ì  ì´ìš© ì‹œ ì €ì‘ìì˜ í—ˆê°€**ê°€ í•„ìš”í•©ë‹ˆë‹¤.  
- ë‹¨, **ì €ì‘ì(ê¹€í˜„êµ¬)**ëŠ” ë³¸ ìë£Œë¥¼ **ììœ ë¡­ê²Œ ìƒì—…ì  ëª©ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

This project is licensed under a **Creative Commons Attribution-NonCommercial 4.0 International License**.  
Non-commercial use and distribution are permitted. **Commercial use requires permission**, except for the original author (**Hyunkoo Kim**), who retains full rights for educational, business, and commercial use.





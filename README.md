# CUDA-30-Days

# ğŸ”¥ CUDA Kernel Study: Conv + Transformer + Attention + SPConv

> 30ì¼ê°„ì˜ CUDA ì»¤ë„ ì§ì ‘ êµ¬í˜„ê³¼ PyTorch/C++/TensorRT/WebGPU ì—°ë™ê¹Œì§€  
> Conv2D, Transformer, Attention, SparseConv í•µì‹¬ ì—°ì‚°ì„ í•œ ë²ˆì— ë§ˆìŠ¤í„°í•©ë‹ˆë‹¤.

---

## ğŸ“Œ ëª©í‘œ

- CUDA ì»¤ë„ ê¸°ë³¸ ê°œë… ë° ì‹¤ìŠµ
- Conv/Transformer/Attention ì»¤ë„ ì§ì ‘ êµ¬í˜„
- Python(PyTorch) ë° C++ ì—°ë™ í…ŒìŠ¤íŠ¸
- PyTorch/ONNX/TensorRT/WebGPU ë¹„êµ ë¶„ì„

---

## ğŸ› ï¸ ì‹¤ìŠµ í™˜ê²½

| êµ¬ì„± | ë²„ì „ |
|------|------|
| CUDA | 12.6 |
| PyTorch | 2.x |
| Python | 3.10+ |
| C++ | CMake ë˜ëŠ” g++ |
| ì„ íƒì‚¬í•­ | ONNXRuntime, TensorRT, WebGPU í™˜ê²½ |

---

## ğŸ—“ï¸ 30ì¼ ì»¤ë¦¬í˜ëŸ¼

| Day | ì»¤ë„ ì´ë¦„ | í•µì‹¬ ë‚´ìš© | ì—°ë™ ë° ë¹„êµ |
|-----|-----------|-----------|---------------|
| 01 | vec_add | ë²¡í„° ë§ì…ˆ | PyTorch + C++ |
| 02 | dot_product | ë‚´ì , reduction | PyTorch + C++ |
| 03 | matrix_add | 2D í–‰ë ¬ í•© | PyTorch + C++ |
| 04 | relu | í™œì„±í™” í•¨ìˆ˜ | PyTorch |
| 05 | matrix_transpose | ì „ì¹˜ | memory coalescing |
| 06 | matmul_basic | naive GEMM | PyTorch |
| 07 | matmul_sharedmem | tiling | PyTorch + ì„±ëŠ¥ |
| 08 | global_avg_pool | GAP | PyTorch ë¹„êµ |
| 09 | batch_norm_forward | ì •ê·œí™” | ONNX export |
| 10 | instance_norm | ì±„ë„ë³„ ì •ê·œí™” | PyTorch |
| 11 | group_norm | ê·¸ë£¹ ì •ê·œí™” | TensorRT plugin |
| 12 | layernorm_forward | LayerNorm FWD | PyTorch |
| 13 | layernorm_backward | LayerNorm BWD | ì„±ëŠ¥ íŠœë‹ |
| 14 | softmax_kernel | softmax | PyTorch |
| 15 | softmax_2d_kernel | Attentionìš© | PyTorch |
| 16 | einsum_qk_attn | QK^T | Attention ì—°ì‚° |
| 17 | attention_softmax_fused | fused attention | íš¨ìœ¨ ë¹„êµ |
| 18 | gelu_kernel | GELU | activation ì„±ëŠ¥ |
| 19 | topk_kernel | Top-K | PyTorch |
| 20 | prefix_sum_kernel | scan | warp-level sum |
| 21 | warp_shuffle_max | warp max | warp shuffle |
| 22 | conv2d_basic | naive Conv | PyTorch |
| 23 | conv2d_shared_tile | tile + shared | PyTorch |
| 24 | conv2d_unroll | unroll ìµœì í™” | ì„±ëŠ¥ ë¹„êµ |
| 25 | spconv_sparse_gemm | sparse matmul | PyTorch |
| 26 | spconv_forward | sparse forward | TensorRT |
| 27 | spconv_backward | grad êµ¬í˜„ | ì‹¤ìŠµ |
| 28 | transformer_block | full block | residual í¬í•¨ |
| 29 | flash_attention | FlashAttention | memory opt |
| 30 | final_review_day | ì „ì²´ ì •ë¦¬ | ì„±ëŠ¥ ë¶„ì„ |

---

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡° ì˜ˆì‹œ
```bash
cuda-kernel-study/
â”œâ”€â”€ README.md
â”œâ”€â”€ day01_vec_add/
â”‚ â”œâ”€â”€ vec_add_kernel.cu
â”‚ â”œâ”€â”€ vec_add_bindings.py
â”‚ â”œâ”€â”€ test_python_runner.py
â”‚ â”œâ”€â”€ test_runner.cpp
â”‚ â””â”€â”€ README.md
â”œâ”€â”€ day02_dot_product/
â”‚ â”œâ”€â”€ ...
```

## âœ… ì‹¤ìŠµ ë°©ì‹

1. CUDA ì»¤ë„ ì§ì ‘ êµ¬í˜„ (`*.cu`)
2. Python(PyTorch) ì—°ë™ (`*.py`)
3. C++ ì‹¤í–‰ ë¹„êµ (`test_runner.cpp`)
4. PyTorch / TensorRT / ONNX / WebGPU ê²°ê³¼ì™€ ë¹„êµ
5. ì„±ëŠ¥ ì¸¡ì • ë° ê²°ê³¼ ì •ë¦¬ (`README.md`)

---

## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ì˜ˆì‹œ

| ì—°ì‚° | PyTorch(ms) | CUDA(ms) | ì„±ëŠ¥ í–¥ìƒ |
|------|-------------|----------|------------|
| matmul | 12.5 | 3.2 | â†‘ 3.9x |
| softmax | 4.3 | 1.8 | â†‘ 2.4x |

---

## ğŸ§  Special Thanks

ì´ ì‹¤ìŠµì€ ì‹¤ì „ AI ì»¤ë„ ì—”ì§€ë‹ˆì–´ë§ì„ ëª©í‘œë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.  
ë‹¤ì–‘í•œ ì—°ì‚°ë“¤ì„ ìµœì í™”í•˜ê³  ì§ì ‘ ë¹„êµí•˜ë©°, ì‹¤ë¬´ì— ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì‹¤ë ¥ì„ ê¸°ë¦…ë‹ˆë‹¤.

---

